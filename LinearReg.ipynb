{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Given a set of observations $x^{(i)}$ and $y^{(i)}$, the objective is to find a function $f: X \\rightarrow Y$ of the form $f(x)=a_{0} + a_{1}x$ \n",
    "that minimizes a loss function $l: f(x^{(i)}), y^{(i)} \\rightarrow L $. \n",
    "\n",
    "In the vectorized form of the function where $a = \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix}$ and $X = \\begin{bmatrix} 1 & x \\end{bmatrix}$ $$f(x)=X \\cdot a$$\n",
    "\n",
    "In this case the loss function is the quadratic loss function $$\\begin{equation} \n",
    "L(a) = \\sum_{i=1}^{N} (y^{(i)}-f(x^{(i)}))^2 = \\sum_{i=1}^{N} (y^{(i)} - a \\cdot X^{(i)})^2 \\end{equation}$$ \n",
    "\n",
    "After the differentiation of the function our purpose is to find the zero of the derivatives\n",
    "$$ \\begin{align*} \n",
    "\\frac{dL}{da} &= - 2 \\sum_{i=1}^{N}(y^{(i)} - a \\cdot X^{(i)} ) \\cdot X^{T (i)} \\\\\n",
    "\\sum_{i=1}^{N} y^{(i)} \\cdot X^{T_{(i)}} &=  \\sum_{i=1}^{N} a \\cdot X^{(i)} \\cdot X^{T_{(i)}} \\\\\n",
    "X^T \\cdot y &= a \\cdot X \\cdot X^T \\\\\n",
    "a &= X^T \\cdot y \\cdot (X \\cdot X^T)^{-1} \n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy \n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Perform linear regression using the conventional way using linear algebra.\n",
    "Args:\n",
    "- X (list): Input feature values.\n",
    "- Y (list): Observed output values.\n",
    "Returns:\n",
    "- list: Parameter vector 'a' for the linear regression model.\n",
    "\"\"\"\n",
    "def linear_regression(X, Y):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    X = np.column_stack((np.ones_like(X), X))\n",
    "    \n",
    "    X_t = np.transpose(X)\n",
    "    XtX_i = np.linalg.inv(np.dot(X_t, X))\n",
    "    a = np.dot(np.dot(XtX_i, X_t), Y)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter vector a: [-0.055  2.035]\n"
     ]
    }
   ],
   "source": [
    "X = [1, 2, 3, 4, 5]\n",
    "Y = [2, 4.05, 6, 8, 10.2]\n",
    "\n",
    "parameters = linear_regression(X, Y)\n",
    "print(\"Parameter vector a:\", parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Given a set of observations $x^{(i)}$ and $y^{(i)}$, the objective is to find a function $f: X \\rightarrow Y$ of the form $f(x) = a_0 + a_1x$ that minimizes a loss function $L$. The loss function quantifies the difference between the predicted values $f(x^{(i)})$ and the actual values $y^{(i)}$.\n",
    "\n",
    "The function can be expressed in a vectorized form where $a = \\begin{bmatrix} a_0 \\\\ a_1 \\end{bmatrix}$ and $X = \\begin{bmatrix} 1 & x \\end{bmatrix}$ for each observation, leading to a prediction model $f(x) = Xa$.\n",
    "\n",
    "The chosen loss function is the quadratic loss function defined as:\n",
    "$$\n",
    "L(a) = \\sum_{i=1}^{N} \\left(y^{(i)} - f(x^{(i)})\\right)^2 = \\sum_{i=1}^{N} \\left(y^{(i)} - X^{(i)}a\\right)^2\n",
    "$$\n",
    "\n",
    "Gradient descent aims to minimize $L(a)$ by iteratively adjusting $a$ in the direction that most rapidly decreases $L(a)$. This is done by computing the gradient of $L(a)$ with respect to $a$ and updating $a$ using the formula:\n",
    "$$\n",
    "a_{\\text{new}} = a_{\\text{old}} - \\alpha \\nabla L(a)\n",
    "$$\n",
    "where $\\alpha$ is the learning rate, a scalar that determines the step size during each iteration.\n",
    "\n",
    "The gradient $\\nabla L(a)$ is calculated as:\n",
    "$$\n",
    "\\nabla L(a) = -2 \\sum_{i=1}^{N} \\left(y^{(i)} - X^{(i)}a \\right)X^{(i)} \n",
    "\n",
    "$$\n",
    "\n",
    "By iteratively applying this update rule, gradient descent seeks to find $a$ that minimizes $L(a)$, effectively training the linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(a, X):\n",
    "    \"\"\"\n",
    "    Predicts the output using a linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - a (numpy array): Coefficients for the linear regression model (a_0, a_1).\n",
    "    - X (list): Input features.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Predicted values.\n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    X = np.column_stack((np.ones_like(X), X))\n",
    "\n",
    "    return np.dot(a,X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(a, X, Y):\n",
    "    Y = np.array(Y)\n",
    "    return (Y - predict(a, X))\n",
    "\n",
    "def quadratic_loss(a, X, Y):\n",
    "    Y = np.array(Y)\n",
    "    return (Y - predict(a, X)) ** 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(a, X, Y):\n",
    "    X_t = np.column_stack((np.ones_like(X), np.array(X))) \n",
    "        \n",
    "    dL_da = - 2 * np.dot(error(a, X, Y), X_t)\n",
    "\n",
    "    return dL_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(a, X, Y, alpha):\n",
    "    a = a - alpha * gradient(a, X, Y)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(a, X, Y, alpha, epochs=None, verbose=100, min_increase=1e-8):\n",
    "    \"\"\"\n",
    "    Trains the linear regression model using gradient descent.\n",
    "\n",
    "    Args:\n",
    "        a (numpy array): Initial parameters.\n",
    "        X (list): Input features.\n",
    "        Y (list): Observed values.\n",
    "        alpha (float): Learning rate.\n",
    "        epochs (int): Number of iterations to run the gradient descent.\n",
    "        verbose (int): Number of times to print loss\n",
    "        min_increase (float): Minimum decrease in error to stop training\n",
    "    Returns:\n",
    "        numpy array: Optimized parameters.\n",
    "    \"\"\"\n",
    "    if epochs is None:\n",
    "        e = 0\n",
    "        increase = 1e100\n",
    "        while increase > min_increase:\n",
    "            error_before = np.sum(quadratic_loss(a, X, Y))\n",
    "            a = update_params(a, X, Y, alpha)\n",
    "            error_after = np.sum(quadratic_loss(a, X, Y))\n",
    "            if e % verbose == 0:\n",
    "                average_loss = np.sum(quadratic_loss(a, X, Y)) / len(Y)\n",
    "                print(f\"Average loss = {average_loss}\")    \n",
    "            increase = error_before - error_after\n",
    "            e += 1\n",
    "        return a\n",
    "    for e in range(epochs):\n",
    "        a = update_params(a, X, Y, alpha)\n",
    "        if e % verbose == 0:\n",
    "            average_loss = np.sum(quadratic_loss(a, X, Y)) / len(Y)\n",
    "            print(f\"Average loss = {average_loss}\")\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss = 0.40432899999999955\n",
      "Average loss = 0.004135393480373686\n",
      "Average loss = 0.0035693447036872277\n",
      "Average loss = 0.0035506392581627446\n",
      "Parameter vector a: [-0.05444144  2.03484529]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0, 1])\n",
    "X = [1, 2, 3, 4, 5]\n",
    "Y = [2, 4.05, 6, 8, 10.2]\n",
    "\n",
    "parameters = train(a, X, Y, alpha=0.01, verbose=100)\n",
    "print(\"Parameter vector a:\", parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
